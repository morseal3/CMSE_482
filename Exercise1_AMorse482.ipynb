{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e4761bd",
   "metadata": {},
   "source": [
    "## 1. Install Anaconda python on your laptop (if you have not done so in the past). Make sure that pandas, json, and matplotlib library packages are installed.\n",
    "#### a. To check what packages are installed, type “conda list” on the command prompt\n",
    "#### b. To install a specific package, say, pandas, type: “conda install pandas”\n",
    "#### If you have not used iPython notebook, read the notes on “Python (Getting Started)” from the resources page on the class website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b027df",
   "metadata": {},
   "source": [
    "completed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bb5759",
   "metadata": {},
   "source": [
    "## 2. Download the data file wiki_edit.csv from the class web page. The data file contains information about revisions that were made to Wikipedia articles in January 2005. Each line corresponds to a revision made by an editor to one of the articles. The format of the space-delimited columns are as follows:\n",
    "#### RevisionId ArticleId Timestamp Editor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a86bb537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "893115cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "column_names = [ 'RevisionId', 'ArticleId', 'Timestamp', 'Editor' ]\n",
    "wiki = pd.read_table(\"wiki_edit.txt\", sep = ' ', header = None, names=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8464ad",
   "metadata": {},
   "source": [
    "### 3. Use pandas read_table function to load the file into a data frame object named data.\n",
    " \n",
    "\n",
    "#### a. Find the top-5 articles that have received the highest number of edits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce9de7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2004_Indian_Ocean_earthquake', 'Humanitarian_response_to_the_2004_Indian_Ocean_earthquake', 'Tsunami', 'Donations_for_victims_of_the_2004_Indian_Ocean_earthquake', 'January_2005']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(wiki['ArticleId'].value_counts()[:5].index.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ba67b6",
   "metadata": {},
   "source": [
    "#### b. Find the top-5 editors who have edited the most number of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9be021a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CanisRufus', 'Jerryseinfeld', 'SimonP', 'Pcpcpc', 'Grutness']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(wiki['Editor'].value_counts()[:5].index.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c11baa2",
   "metadata": {},
   "source": [
    "### 4. Download the JSON data file wh.json from the class web page. The data corresponds to tweet messages posted by the White House. Find the top 15 most frequent terms that appear in the tweet messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1348584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "whj = open('wh.json')\n",
    "  \n",
    "wh = [json.loads(line) for line in whj]\n",
    "whj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31b6cff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "count = defaultdict(int)\n",
    "for t in wh:\n",
    "    ws = t['text'].split(' ')\n",
    "    for w in ws:\n",
    "        count[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f84653a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('my', 7),\n",
       " ('you', 8),\n",
       " ('for', 8),\n",
       " ('@WhiteHouse:', 8),\n",
       " ('@vj44:', 9),\n",
       " ('a', 9),\n",
       " ('and', 9),\n",
       " ('in', 9),\n",
       " ('@POTUS', 9),\n",
       " ('that', 9),\n",
       " ('our', 10),\n",
       " ('of', 22),\n",
       " ('to', 27),\n",
       " ('the', 28),\n",
       " ('RT', 34)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator \n",
    "sorted_w = sorted(count.items(), key = operator.itemgetter(1))\n",
    "sorted_w[-15:]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
